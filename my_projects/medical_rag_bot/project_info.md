
# ğŸ©º Medical GenAI App â€” Final Project Documentation

## ğŸ“Œ Project Overview

**medical_genai_app** is a **Retrieval-Augmented Generation (RAG)** based medical chatbot that:

* Ingests medical PDFs
* Converts text into embeddings
* Stores embeddings in FAISS
* Retrieves relevant medical context
* Uses an LLM to answer user questions
* Provides a Streamlit-based UI

The project follows **clean architecture** with **strict separation** between:

* UI (`app/`)
* Backend RAG logic (`core/`)
* Persistent data (`data/`)

---

## ğŸ“ Final Folder Structure (Corrected)

```
medical_genai_app/
â”œâ”€ .venv/                     # Virtual environment
â”œâ”€ data/
â”‚   â””â”€ faiss_index/           # Persistent FAISS index
â”‚       â”œâ”€ index.faiss
â”‚       â””â”€ index.pkl
â””â”€ src/
   â”œâ”€ app/                    # Streamlit frontend
   â”‚   â”œâ”€ streamlit_app.py
   â”‚   â”œâ”€ chat_ui_logic.py
   â”‚   â”œâ”€ pdf_uploader.py
   â”‚   â””â”€ ui_components.py
   â”‚
   â””â”€ core/                   # RAG backend
       â”œâ”€ config/
       â”‚   â””â”€ settings.py
       â”œâ”€ embeddings/
       â”‚   â””â”€ faiss_index.py
       â”œâ”€ ingestion/
       â”‚   â”œâ”€ pdf_parser.py
       â”‚   â””â”€ ingest.py
       â”œâ”€ query/
       â”‚   â”œâ”€ query_index.py
       â”‚   â””â”€ chat_model.py
       â””â”€ utils/
```

---

# ğŸ–¥ï¸ Frontend Layer â€” `src/app/`

This layer **never touches FAISS directly**.
It only calls **backend APIs**.

---

## 1ï¸âƒ£ `streamlit_app.py` â€” Application Entry Point

### Purpose

* Main Streamlit entry file
* Wires together UI components and chat logic

### Responsibilities

* Configure Streamlit
* Render header
* Render PDF uploader
* Render chat interface

### Typical Flow

```text
App start
 â†’ Header
 â†’ PDF upload (optional)
 â†’ Chat UI
```

### Calls

* `render_header()` â†’ `ui_components.py`
* `pdf_uploader()` â†’ `pdf_uploader.py`
* `render_chat_ui()` â†’ `chat_ui_logic.py`

---

## 2ï¸âƒ£ `pdf_uploader.py` â€” Upload & Re-Index PDFs

### Purpose

* Accepts PDF uploads from the user
* Triggers ingestion automatically

### Responsibilities

* Upload PDFs via Streamlit
* Save them to a temporary folder
* Call `ingest_pdfs()` from backend

### Key Functions

| Function                | Description             |
| ----------------------- | ----------------------- |
| `pdf_uploader()`        | UI + upload handling    |
| `save_uploaded_files()` | Saves uploaded PDFs     |
| `trigger_ingestion()`   | Calls backend ingestion |

ğŸ“Œ **This is the only place ingestion is triggered from UI**

---

## 3ï¸âƒ£ `chat_ui_logic.py` â€” Chat UI & State

### Purpose

* Handles chat interaction logic
* Maintains conversation state

### Responsibilities

* Manage `st.session_state.messages`
* Display chat history
* Send user question to backend
* Display LLM response

### Key Function

| Function           | Description    |
| ------------------ | -------------- |
| `render_chat_ui()` | Main chat loop |

---

## 4ï¸âƒ£ `ui_components.py` â€” Reusable UI Elements

### Purpose

* Keep UI clean and modular

### Examples

* Page headers
* Dividers
* Status messages

ğŸ“Œ **No business logic here**

---

# ğŸ§  Backend Layer â€” `src/core/`

This layer contains **all RAG intelligence**.

---

## âš™ï¸ Configuration â€” `core/config/`

### `settings.py`

### Purpose

* Central configuration file

### Contains

* API keys
* Base directory paths
* FAISS index directory
* Model names
* Chunk sizes

ğŸ“Œ **Single source of truth**

---

## ğŸ§¬ Embeddings & Vector Store â€” `core/embeddings/`

### `faiss_index.py`

### Purpose

* Handles FAISS index lifecycle
* Ensures embedding consistency

### Key Functions

| Step     | Function                               | Description           |
| -------- | -------------------------------------- | --------------------- |
| Shared   | `get_embeddings()`                     | Loads embedding model |
| Step 3   | `create_faiss_index(texts)`            | Creates FAISS index   |
| Step 4.1 | `load_faiss_index()`                   | Loads index from disk |
| Step 4.2 | `retrieve_similar_documents(query, k)` | Vector search         |

ğŸ“Œ **Same embedding model used for ingestion & query**

---

## ğŸ“¥ Ingestion Pipeline â€” `core/ingestion/`

### `pdf_parser.py`

### Purpose

* Extracts raw text from PDFs

### Key Function

| Function                      | Description |
| ----------------------------- | ----------- |
| `extract_text_from_pdf(path)` | PDF â†’ text  |

---

### `ingest.py`

### Purpose

* Orchestrates **PDF â†’ FAISS**

### Step-by-Step Functions

| Step | Function         | Description             |
| ---- | ---------------- | ----------------------- |
| 3.1  | `get_all_pdfs()` | Scan PDF directory      |
| 3.2  | `chunk_text()`   | Split text into chunks  |
| 3.3  | `ingest_pdfs()`  | Full ingestion pipeline |

### What `ingest_pdfs()` Does

1. Finds PDFs
2. Extracts text
3. Chunks text
4. Creates FAISS index
5. Saves index to `data/faiss_index/`

---

## ğŸ” Query Layer â€” `core/query/`

### `query_index.py`

### Purpose

* Bridge between user query and FAISS

### Key Function

| Function                 | Description             |
| ------------------------ | ----------------------- |
| `search_faiss(query, k)` | Returns relevant chunks |

ğŸ“Œ **No LLM calls here**

---

### `chat_model.py`

### Purpose

* Generates final answer using RAG

### Responsibilities

1. Receive user question
2. Retrieve relevant chunks
3. Build RAG prompt
4. Call LLM
5. Return answer

### Key Function

| Function                    | Description      |
| --------------------------- | ---------------- |
| `generate_answer(question)` | RAG-based answer |

ğŸ“Œ Enforces:

> â€œAnswer ONLY using provided medical documents.â€

---

## ğŸ› ï¸ Utilities â€” `core/utils/`

### Purpose

* Shared helper utilities
* No core logic

(Currently optional / extensible)

---

# ğŸ”„ End-to-End Workflow (Simple View)

| Stage           | File                | Responsibility |
| --------------- | ------------------- | -------------- |
| Upload PDFs     | `pdf_uploader.py`   | User uploads   |
| Extract text    | `pdf_parser.py`     | PDF â†’ text     |
| Chunk text      | `ingest.py`         | Text â†’ chunks  |
| Embed           | `faiss_index.py`    | Text â†’ vectors |
| Store           | `data/faiss_index/` | Persist index  |
| Ask question    | `chat_ui_logic.py`  | User input     |
| Retrieve        | `query_index.py`    | Similar chunks |
| Generate answer | `chat_model.py`     | LLM response   |
| Display         | `chat_ui_logic.py`  | UI output      |

---

# âœ… Why This Architecture Is Strong

âœ” Clean separation of concerns
âœ” UI never touches FAISS
âœ” Ingestion independent of querying
âœ” Easily extensible (memory, citations, streaming)
âœ” Production-grade RAG structure

---
