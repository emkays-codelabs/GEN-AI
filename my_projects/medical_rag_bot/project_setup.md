

---

# ğŸ©º Medical GenAI App â€” Poetry Project Setup Guide

## ğŸ“Œ Project Summary

**Medical GenAI App** is a **Retrieval-Augmented Generation (RAG)** system that enables users to:

* Upload medical PDFs
* Create a persistent FAISS vector index
* Ask natural language questions
* Receive LLM-generated answers grounded in documents

The project follows:

* **Clean Architecture**
* **Strict UI / Backend separation**
* **Persistent vector storage**
* **Reproducible dependency management using Poetry**

---

## ğŸš€ Why Poetry for This Project?

Poetry is ideal here because:

âœ” Deterministic dependency resolution
âœ” Clean virtual environment isolation
âœ” Easy collaboration
âœ” Clear distinction between dev and prod deps
âœ” Python path safety for `src/` layout

This project **benefits heavily** from Poetry because it uses:

* LangChain ecosystem
* FAISS
* Transformers
* Streamlit
* LLM providers

---

## ğŸ“ Validated Project Structure

```
medical_genai_app/
â”œâ”€ pyproject.toml              # Poetry configuration
â”œâ”€ poetry.lock                 # Locked dependency graph
â”œâ”€ .env                        # Environment variables
â”œâ”€ data/
â”‚   â””â”€ faiss_index/            # Persistent FAISS index
â”‚       â”œâ”€ index.faiss
â”‚       â””â”€ index.pkl
â””â”€ src/
   â”œâ”€ app/                     # Streamlit frontend
   â”‚   â”œâ”€ streamlit_app.py
   â”‚   â”œâ”€ chat_ui_logic.py
   â”‚   â”œâ”€ pdf_uploader.py
   â”‚   â””â”€ ui_components.py
   â”‚
   â””â”€ core/                    # RAG backend
       â”œâ”€ config/
       â”‚   â””â”€ settings.py
       â”œâ”€ embeddings/
       â”‚   â””â”€ faiss_index.py
       â”œâ”€ ingestion/
       â”‚   â”œâ”€ pdf_parser.py
       â”‚   â””â”€ ingest.py
       â”œâ”€ query/
       â”‚   â”œâ”€ query_index.py
       â”‚   â””â”€ chat_model.py
       â””â”€ utils/
```

ğŸ“Œ This is a **proper `src/` layout**, fully compatible with Poetry.

---

## ğŸ§± Step 1: Install Poetry

```bash
pip install poetry
```

Verify:

```bash
poetry --version
```

---

## ğŸ§± Step 2: Initialize Poetry Project

From project root:

```bash
poetry init
```

Key choices:

* **Package name**: `medical-genai-app`
* **Python version**: `>=3.10,<3.13`
* **Dependencies**: add later

This creates:

* `pyproject.toml`

---

## ğŸ“¦ Step 3: Add Dependencies

### Core Dependencies

```bash
poetry add \
  streamlit \
  langchain \
  langchain-community \
  sentence-transformers \
  faiss-cpu \
  pypdf \
  python-dotenv
```

### If using EuriAI / custom provider

```bash
poetry add euriai
```

### Development Dependencies

```bash
poetry add --group dev black ruff pytest ipykernel
```

---

## ğŸ§  Dependency Rationale

| Package               | Why                |
| --------------------- | ------------------ |
| streamlit             | Web UI             |
| langchain             | RAG orchestration  |
| langchain-community   | FAISS, embeddings  |
| sentence-transformers | Embeddings         |
| faiss-cpu             | Vector search      |
| pypdf                 | PDF parsing        |
| python-dotenv         | Environment config |
| euriai                | Chat model backend |
| pytest                | Testing            |
| black / ruff          | Code quality       |

---

## ğŸ” Step 4: Environment Variables

Create `.env` in root:

```env
EURI_API_KEY=your_api_key_here
```

Load it in `settings.py`:

```python
from dotenv import load_dotenv
load_dotenv()
```

---

## âš™ï¸ Step 5: `settings.py` (Critical)

Central configuration file:

```python
from pathlib import Path
import os

BASE_DIR = Path(__file__).resolve().parents[3]

DATA_DIR = BASE_DIR / "data"
FAISS_INDEX_DIR = DATA_DIR / "faiss_index"

EURI_API_KEY = os.getenv("EURI_API_KEY")

CHUNK_SIZE = 500
CHUNK_OVERLAP = 50
TOP_K = 4
```

ğŸ“Œ **Every module imports paths from here â€” never hardcode paths**

---

## ğŸ§¬ Step 6: Ingestion Workflow (One-Time or On Upload)

Triggered by:

* `pdf_uploader.py`

Flow:

```
PDF Upload
 â†’ temp folder
 â†’ extract_text_from_pdf()
 â†’ chunk_text()
 â†’ create_faiss_index()
 â†’ save_local(data/faiss_index)
```

This happens **outside chat runtime**.

---

## ğŸ” Step 7: Query Workflow (Runtime)

Triggered by:

* Chat input in Streamlit

Flow:

```
User Question
 â†’ search_faiss()
 â†’ retrieve_similar_documents()
 â†’ generate_answer()
 â†’ display response
```

ğŸ“Œ FAISS is **loaded from disk**, not rebuilt.

---

## ğŸ§ª Step 8: Running the App (Poetry Way)

### Activate environment

```bash
poetry shell
```

### Run Streamlit

```bash
streamlit run src/app/streamlit_app.py
```

OR

```bash
poetry run streamlit run src/app/streamlit_app.py
```

---

## âœ… Key Architectural Checkpoints

âœ” Same embedding model for ingest + query
âœ” FAISS index persists across restarts
âœ” UI does not import FAISS directly
âœ” LLM only sees retrieved chunks
âœ” Config centralized
âœ” No hardcoded paths
âœ” No global state outside Streamlit session

---

## âš ï¸ Common Pitfalls (Avoid These)

âŒ Rebuilding FAISS on every question
âŒ Different embedding models for ingest/query
âŒ Hardcoding `"data/faiss_index"`
âŒ LLM answering without context guardrails
âŒ Mixing UI and RAG logic

---

## ğŸ§­ Recommended Next Enhancements

* Source citations in UI
* Conversation-aware retrieval
* Streaming responses
* Docker + Poetry export
* Multi-document indexing

---

## ğŸ Final Note

This is **production-quality RAG structure**.
With Poetry + this architecture, you have:

> âœ” Reproducibility
> âœ” Scalability
> âœ” Clarity
> âœ” Maintainability
---
