

# ğŸ“˜ Multimodal Vector Search using ChromaDB

**Dataset:** *â€œAttention Is All You Needâ€* (Transformer paper)

---

## ğŸ” What are we trying to build?

We want a system where you can ask questions like:

> *â€œWhat is the transformer architecture?â€*
> *â€œShow me the attention diagramâ€*

â€¦and the system will return:

* **Relevant text passages**
* **Relevant images (figures, diagrams)**
  from a **research paper PDF**.

This is called **multimodal retrieval** (text + images).

---

## ğŸ§  Core Idea (Big Picture)

Instead of keyword search:

* Convert **text â†’ vectors**
* Convert **images â†’ vectors**
* Store vectors in a **vector database**
* Search using **semantic similarity**

---

## ğŸ§© High-Level Architecture

```
PDF
 â”‚
 â”œâ”€â”€ Text chunks â”€â”€â–º Text Embeddings â”€â”€â–º Text Vector Index
 â”‚
 â””â”€â”€ Images â”€â”€â”€â”€â”€â”€â”€â–º Image Embeddings â”€â–º Image Vector Index
                           â–²
                           â”‚
                    CLIP Model
                           â”‚
User Query (text) â”€â”€â–º Query Embedding
                           â”‚
                 Semantic Search (Similarity)
                           â”‚
                   Ranked Results (Text + Images)
```

---

## ğŸ› ï¸ Technologies Used

| Component                        | Purpose                        |
| -------------------------------- | ------------------------------ |
| **PyMuPDF (fitz)**               | Extract text & images from PDF |
| **Pillow (PIL)**                 | Image handling                 |
| **Sentence-Transformers (CLIP)** | Multimodal embeddings          |
| **ChromaDB**                     | Vector database                |
| **Hugging Face**                 | Model hosting                  |

---

## 1ï¸âƒ£ Why Vector Databases?

Traditional DBs store:

* strings
* numbers

Vector DBs store:

* **embeddings** (arrays of numbers)
* allow **semantic similarity search**

### Examples:

* ChromaDB
* FAISS
* Pinecone
* Weaviate
* Qdrant

---

## 2ï¸âƒ£ Why CLIP?

CLIP (**Contrastive Languageâ€“Image Pretraining**) maps:

* text ğŸ“
* images ğŸ–¼ï¸

into **the same vector space**.

That means:

> Text query can retrieve images
> Image can retrieve text

Perfect for multimodal search.

---

## 3ï¸âƒ£ Installation

```bash
pip install pymupdf sentence-transformers chromadb pillow numpy
```

---

## 4ï¸âƒ£ Imports & Setup

```python
import os
import fitz  # PyMuPDF
import numpy as np
from PIL import Image
from typing import List, Dict, Any

from sentence_transformers import SentenceTransformer
import chromadb
```

---

## 5ï¸âƒ£ Load CLIP Model (from Hugging Face)

```python
clip_model = SentenceTransformer("clip-ViT-B-32")
```

### What this model does:

* `encode(text)` â†’ text embedding
* `encode(image)` â†’ image embedding
* Output: same vector dimension

---

## 6ï¸âƒ£ Embedding Functions (CRITICAL)

### Text Embedding

```python
def embed_text(text: str) -> np.ndarray:
    emb = clip_model.encode([text], convert_to_numpy=True)
    return emb[0]
```

### Image Embedding

```python
def embed_image(path: str) -> np.ndarray:
    img = Image.open(path).convert("RGB")
    emb = clip_model.encode([img], convert_to_numpy=True)
    return emb[0]
```

âœ… **Same vector space**
âœ… **Same embedding size**

---

## 7ï¸âƒ£ Extract Text & Images from PDF

### Open PDF

```python
PDF_PATH = "attention is all you need.pdf"
doc = fitz.open(PDF_PATH)
```

---

### Extract Text Blocks

```python
text_chunks = []

for page_num, page in enumerate(doc):
    blocks = page.get_text("blocks")
    for b in blocks:
        text = b[4].strip()
        if len(text) > 50:
            text_chunks.append({
                "page": page_num,
                "text": text
            })
```

ğŸ“Œ **Why blocks?**
Blocks preserve layout better than raw text.

---

### Extract Images

```python
FIGURES_DIR = "figures"
os.makedirs(FIGURES_DIR, exist_ok=True)

image_metadata = []

for page_num, page in enumerate(doc):
    images = page.get_images(full=True)
    for img_index, img in enumerate(images):
        xref = img[0]
        base_image = doc.extract_image(xref)
        image_bytes = base_image["image"]
        image_ext = base_image["ext"]

        image_path = f"{FIGURES_DIR}/page{page_num}_img{img_index}.{image_ext}"
        with open(image_path, "wb") as f:
            f.write(image_bytes)

        image_metadata.append({
            "page": page_num,
            "path": image_path
        })
```

---

## 8ï¸âƒ£ Caption Extraction Logic (Conceptual)

ğŸ“Œ Captions usually:

* Appear **below images**
* Start with â€œFigure Xâ€

Logic:

1. Get image bounding box
2. Find nearest text block below
3. Assign as caption

(This is heuristic-based and imperfect â€” but practical)

---

## 9ï¸âƒ£ Initialize ChromaDB

```python
client = chromadb.Client()
```

---

## ğŸ”Ÿ Create Two Separate Indexes

Why two?

| Index           | Reason                   |
| --------------- | ------------------------ |
| **Text Index**  | Optimized for paragraphs |
| **Image Index** | Optimized for figures    |

```python
text_collection = client.create_collection("paper_text")
image_collection = client.create_collection("paper_images")
```

---

## 1ï¸âƒ£1ï¸âƒ£ Store Text Embeddings

```python
for i, chunk in enumerate(text_chunks):
    emb = embed_text(chunk["text"])
    text_collection.add(
        ids=[f"text_{i}"],
        embeddings=[emb],
        documents=[chunk["text"]],
        metadatas=[{"page": chunk["page"]}]
    )
```

---

## 1ï¸âƒ£2ï¸âƒ£ Store Image Embeddings

```python
for i, img in enumerate(image_metadata):
    emb = embed_image(img["path"])
    image_collection.add(
        ids=[f"img_{i}"],
        embeddings=[emb],
        metadatas=[{
            "page": img["page"],
            "path": img["path"]
        }]
    )
```

---

## 1ï¸âƒ£3ï¸âƒ£ Query Function (THE HEART)

```python
def answer_query(
    query: str,
    top_k_text: int = 5,
    top_k_img: int = 5,
    top_k_overall: int = 8
) -> List[Dict[str, Any]]:

    q_emb = embed_text(query)

    text_results = text_collection.query(
        query_embeddings=[q_emb],
        n_results=top_k_text
    )

    img_results = image_collection.query(
        query_embeddings=[q_emb],
        n_results=top_k_img
    )

    combined = []

    for i in range(len(text_results["ids"][0])):
        combined.append({
            "type": "text",
            "content": text_results["documents"][0][i],
            "distance": text_results["distances"][0][i]
        })

    for i in range(len(img_results["ids"][0])):
        combined.append({
            "type": "image",
            "path": img_results["metadatas"][0][i]["path"],
            "distance": img_results["distances"][0][i]
        })

    combined.sort(key=lambda x: x["distance"])
    return combined[:top_k_overall]
```

---

## 1ï¸âƒ£4ï¸âƒ£ Example Query

```python
answer_query(
    "what is transformer architecture",
    top_k_text=2,
    top_k_img=1,
    top_k_overall=3
)
```

### Output:

* Text explaining self-attention
* Transformer block diagram image

---

## ğŸ” Workflow Summary Diagram

```
PDF
 â”œâ”€â”€ Extract Text â”€â”€â–º Embed â”€â”€â–º Text Vector DB
 â”œâ”€â”€ Extract Images â”€â–º Embed â”€â”€â–º Image Vector DB
 â”‚
User Query
 â””â”€â”€ Embed Query
       â”œâ”€â”€ Search Text DB
       â”œâ”€â”€ Search Image DB
       â””â”€â”€ Merge + Rank Results
```

---

## ğŸš€ Model Improvements

Suggested upgrades:

* **EVA-CLIP** â†’ better image understanding
* **BLIP / BLIP-2** â†’ caption-aware embeddings
* **LayoutLM** â†’ better PDF structure understanding

---

## ğŸ”„ Alternative Vector Databases

| DB       | Strength        |
| -------- | --------------- |
| FAISS    | Fast local      |
| Pinecone | Managed cloud   |
| Weaviate | Schema + Graph  |
| Qdrant   | Rust, very fast |

---

## ğŸ§  Key Concepts You Learned

* What embeddings are
* Why vector databases matter
* How multimodal models work
* How to extract structured data from PDFs
* How RAG systems are built internally

---

## ğŸ¯ Key Takeaways

âœ… **Vector search beats keyword search**
âœ… **CLIP enables cross-modal retrieval**
âœ… **Data preparation matters more than models**
âœ… **Separate indexes improve quality**
âœ… **This is the foundation of ChatPDF systems**

---

## ğŸ”® Whatâ€™s Next (Project)

Next class project:

* Multiple PDFs
* Chat interface
* Source citations
* CI/CD pipeline
* Production-ready RAG system

If you want, next I can:

* Draw this as a **proper architecture diagram**
* Convert this into a **README.md**
* Debug your current notebook line-by-line
* Extend this into a **chatbot UI**

Just tell me ğŸ‘

