
# üìò Multimodal Vector Search using ChromaDB

### Text + Image Semantic Retrieval from Research PDFs

**Dataset:** *‚ÄúAttention Is All You Need‚Äù* (Transformer paper)

---

## 1Ô∏è‚É£ What Are We Trying to Build?

We are building a system that allows **natural language questions** over a research PDF, such as:

* *‚ÄúWhat is the transformer architecture?‚Äù*
* *‚ÄúShow me the attention diagram‚Äù*

‚Ä¶and returns:

* üìÑ **Relevant text passages**
* üñºÔ∏è **Relevant images (figures, diagrams)**

This is called **multimodal semantic retrieval** ‚Äî retrieving **text + images** using **meaning**, not keywords.

---

## 2Ô∏è‚É£ Core Idea (Big Picture)

Traditional search:

* Keyword-based
* Exact word matching
* No understanding of meaning

Our system:

1. Convert **text ‚Üí vectors**
2. Convert **images ‚Üí vectors**
3. Store vectors in a **vector database**
4. Search using **semantic similarity**

> If two things mean the same thing, their vectors will be close ‚Äî even if the words differ.

---

## 3Ô∏è‚É£ High-Level Architecture

```
PDF
 ‚îÇ
 ‚îú‚îÄ‚îÄ Text chunks ‚îÄ‚îÄ‚ñ∫ Text Embeddings ‚îÄ‚îÄ‚ñ∫ Text Vector Index
 ‚îÇ
 ‚îî‚îÄ‚îÄ Images ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Image Embeddings ‚îÄ‚ñ∫ Image Vector Index
                           ‚ñ≤
                           ‚îÇ
                    CLIP Model
                           ‚îÇ
User Query (text) ‚îÄ‚îÄ‚ñ∫ Query Embedding
                           ‚îÇ
                 Semantic Search (Similarity)
                           ‚îÇ
                   Ranked Results (Text + Images)
```

### Key Insight

* Text and images are **separate modalities**
* A **single model (CLIP)** connects them
* One text query can retrieve **both text and images**

---

## 4Ô∏è‚É£ Technologies Used

| Component                    | Purpose                    |
| ---------------------------- | -------------------------- |
| PyMuPDF (fitz)               | Layout-aware PDF parsing   |
| Pillow (PIL)                 | Image loading & processing |
| Sentence-Transformers (CLIP) | Multimodal embeddings      |
| ChromaDB (Cloud)             | Vector database            |
| Hugging Face                 | Model hosting              |

---

## 5Ô∏è‚É£ Why Vector Databases?

Traditional databases store:

* Strings
* Numbers
* Rows & columns

Vector databases store:

* **Embeddings** (high-dimensional vectors)
* Optimized similarity indexes (ANN)

They enable:

* Semantic search
* Fast similarity queries
* Scalable retrieval

### Popular Vector Databases

* ChromaDB
* FAISS
* Pinecone
* Weaviate
* Qdrant

---

## 6Ô∏è‚É£ Why CLIP?

CLIP (**Contrastive Language‚ÄìImage Pretraining**) embeds:

* üìù Text
* üñºÔ∏è Images

into the **same vector space**.

### Why this matters

* Text ‚Üí Image retrieval
* Image ‚Üí Text retrieval
* Cross-modal semantic search

This is the **core enabler** of multimodal systems.

---

## 7Ô∏è‚É£ Project Structure (Fully Explained)

```
chroma_multimodal_app/
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ attention_is_all_you_need.pdf
‚îú‚îÄ‚îÄ figures/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ core/
‚îÇ       ‚îú‚îÄ‚îÄ config/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ settings.py
‚îÇ       ‚îú‚îÄ‚îÄ embeddings/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ clip_embeddings.py
‚îÇ       ‚îú‚îÄ‚îÄ ingestion/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ pdf_parser.py
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ ingest.py
‚îÇ       ‚îú‚îÄ‚îÄ query/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ query_index.py
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ ask.py
‚îÇ       ‚îú‚îÄ‚îÄ utils/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ chroma_client.py
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ file_utils.py
‚îÇ       ‚îî‚îÄ‚îÄ main.py
‚îî‚îÄ‚îÄ tests/
```

### Why this structure?

* Clear separation of concerns
* Scales cleanly to large systems
* Production-grade organization

---

## 8Ô∏è‚É£ Configuration & Environment

### `.env`

Stores secrets:

* Chroma API key
* Tenant
* Database name

### `settings.py`

Centralized configuration:

* PDF path
* Figures directory
* Model name (`clip-ViT-B-32`)
* Collection names
* Text length thresholds

---

## 9Ô∏è‚É£ PDF Parsing (Why Layout Matters)

We extract text using:

```python
page.get_text("blocks")
```

Each block contains:

* Bounding box `(x0, y0, x1, y1)`
* Text content

### Why blocks?

* Preserve layout
* Maintain paragraphs
* Enable caption detection
* Support explainability

---

## üîü Text Chunk Extraction

### Steps

1. Iterate pages
2. Extract text blocks
3. Clean text
4. Skip short/noisy chunks
5. Store metadata

### Metadata Stored

* Page number
* Bounding box
* Original text

This enables grounding and UI highlighting.

---

## 1Ô∏è‚É£1Ô∏è‚É£ Image Extraction

Using:

```python
page.get_images(full=True)
```

### Steps

1. Extract image bytes
2. Save to `/figures`
3. Record bounding boxes
4. Store metadata

Images become **searchable entities**, not just files.

---

## 1Ô∏è‚É£2Ô∏è‚É£ Caption Extraction Logic

### Why captions matter

Images alone lack semantic meaning.

### Heuristic

1. Get image bounding box
2. Find text blocks **below** the image
3. Choose the closest vertically
4. Assign as caption

This heuristic is imperfect but **works well for academic PDFs**.

---

## 1Ô∏è‚É£3Ô∏è‚É£ CLIP Embeddings

### Model

```
clip-ViT-B-32
```

### Embedding Functions

* `embed_text(text)`
* `embed_image(image_path)`

Both produce vectors in:

* Same dimension
* Same semantic space

---

## 1Ô∏è‚É£4Ô∏è‚É£ ChromaDB Collections

Two collections:

| Collection | Purpose                   |
| ---------- | ------------------------- |
| Text       | Paragraph-level semantics |
| Images     | Visual semantics          |

Stored in **Chroma Cloud**.

---

## 1Ô∏è‚É£5Ô∏è‚É£ Ingestion Pipeline

Implemented in `ingest.py`.

### Steps

1. Load PDF
2. Extract text & images
3. Generate embeddings
4. Upload to ChromaDB with metadata

This is an **offline process**.

---

## 1Ô∏è‚É£6Ô∏è‚É£ Query Pipeline (Core Logic)

Implemented in `query_index.py`.

### Flow

1. User query ‚Üí text
2. Embed query using CLIP
3. Search text collection
4. Search image collection
5. Merge results
6. Rank by similarity

---

## 1Ô∏è‚É£7Ô∏è‚É£ CLI Query Interface

`ask.py` provides:

* Command-line querying
* Debugging & demos
* Developer-friendly testing

---

## 1Ô∏è‚É£8Ô∏è‚É£ End-to-End Workflow

```
[Ingestion]
PDF ‚Üí parse ‚Üí embed ‚Üí ChromaDB

[Query]
Query ‚Üí embed ‚Üí search ‚Üí merge ‚Üí rank ‚Üí results
```

---

## 1Ô∏è‚É£9Ô∏è‚É£ Architecture Patterns Used

* Multimodal indexing
* Vector similarity search
* Metadata-aware retrieval
* Retrieval-Augmented Generation (RAG foundation)

---

## 2Ô∏è‚É£0Ô∏è‚É£ Model Improvements (Suggested Upgrades)

| Model         | Benefit                            |
| ------------- | ---------------------------------- |
| EVA-CLIP      | Better image understanding         |
| BLIP / BLIP-2 | Caption-aware embeddings           |
| LayoutLM      | Strong PDF structure understanding |

The system is **model-agnostic**.

---

## 2Ô∏è‚É£1Ô∏è‚É£ Alternative Vector Databases

| Database | Strength          |
| -------- | ----------------- |
| FAISS    | Fast local search |
| Pinecone | Managed cloud     |
| Weaviate | Schema + graph    |
| Qdrant   | Rust, very fast   |

---

## 2Ô∏è‚É£2Ô∏è‚É£ Key Concepts You Learned

* Embeddings & vector spaces
* Semantic vs keyword search
* Multimodal retrieval
* PDF layout understanding
* Metadata-driven AI systems
* Real-world RAG architectures

---

## 2Ô∏è‚É£3Ô∏è‚É£ Key Takeaways

‚úÖ Vector search beats keyword search
‚úÖ CLIP enables cross-modal retrieval
‚úÖ Data preparation matters more than models
‚úÖ Separate indexes improve quality
‚úÖ This is the foundation of **ChatPDF systems**

---

## 2Ô∏è‚É£4Ô∏è‚É£ Final Mental Model

> Everything becomes vectors.
> Queries become vectors.
> Similar vectors mean similar meaning.

---

## 2Ô∏è‚É£5Ô∏è‚É£ Why This Project Matters

This architecture is used in:

* ChatPDF systems
* Enterprise document search
* Legal & medical RAG pipelines
* Multimodal AI assistants

